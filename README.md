# Climate Analytics Platform

![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.5-orange.svg)
![Kubernetes](https://img.shields.io/badge/Kubernetes-v1.28-blue.svg)
![Docker](https://img.shields.io/badge/Docker-20.10%2B-blue.svg)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-13-blue.svg)
![MinIO](https://img.shields.io/badge/MinIO-latest-red.svg)

### Project Objective
This project aims to design and implement an end-to-end data engineering pipeline for comprehensive analysis of global climate change patterns using containerized distributed systems.

### Team
*   **JoÃ£o Antunes** (2021220964)
*   **Gabriel Pinto** (2021220925)
*   **JosÃ© Cunha** (2021223719)

### Course
**Advanced Infrastructures for Data Science (IACD)** - MIACD 2025/2026
University of Coimbra

---

## ğŸ›ï¸ Architecture

The platform is built around a Lambda-like architecture, separating real-time and batch processing.

```mermaid
flowchart LR
    A[Data Sources] -->|Real-time Ingestion| B[Spark Streaming]
    B -->|Persistent Raw Storage| C[MinIO Storage]
    C -->|Batch Processing| D[Spark Processing]
    D -->|Structured Storage| E[PostgreSQL]
    E -->|Interactive Visualization| F[Grafana Dashboard]
```

### Data Flow Explanation

*   **Data Sources â†’ Spark Streaming:** Real-time ingestion of climate data from NASA and Global Carbon Project sources.
*   **Spark Streaming â†’ MinIO Storage:** Raw data is persistently stored in object storage for reliability and reprocessing capability.
*   **MinIO Storage â†’ Spark Processing:** Batch processing loads data for analytical transformations.
*   **Spark Processing â†’ PostgreSQL:** Processed results and aggregated metrics are stored in a structured format for efficient querying.
*   **PostgreSQL â†’ Grafana Dashboard:** Final visualization layer queries the database to display interactive climate analytics.

---

## ğŸš€ Getting Started

### Prerequisites

Ensure you have the following software installed on your system:

*   **Docker** (with Kubernetes enabled)
*   **Git**
*   **Python** (3.9 or higher)
*   **kubectl**

### Installation

This project includes detailed, step-by-step setup guides for different operating systems. Please follow the guide that matches your environment.

*   â¡ï¸ **For Windows users:** See [**docs/SETUP_WINDOWS.md**](./docs/SETUP_WINDOWS.md)
*   â¡ï¸ **For Linux users:** See [**docs/SETUP_LINUX.md**](./docs/SETUP_LINUX.md)

The setup guides will walk you through installing dependencies, building the application image, and deploying the entire infrastructure on Kubernetes.

---

## âš™ï¸ How to Run

The primary method for running this project is via the Kubernetes manifests as described in the setup guides.

### Alternative: Docker Compose (for local development)

For a simpler, non-Kubernetes local setup, you can use the provided `docker-compose.yml` file. This is ideal for development and testing individual services.

```bash
# Start all services (Spark, MinIO, PostgreSQL, Grafana) in the background
docker-compose up -d

# To stop all services
docker-compose down
```
**Note:** When using Docker Compose, you will need to manually submit the Spark jobs using `docker exec` and run the MinIO initialization script. The Kubernetes setup automates most of these steps.

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ data/                  # Raw data files (not checked into git)
â”œâ”€â”€ docs/                  # Setup and installation guides
â”‚   â”œâ”€â”€ SETUP_LINUX.md
â”‚   â””â”€â”€ SETUP_WINDOWS.md
â”œâ”€â”€ jars/                  # JAR dependencies for Spark (e.g., S3, Postgres drivers)
â”œâ”€â”€ k8s/                   # Kubernetes deployment manifests
â”‚   â””â”€â”€ deployment.yml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/         # Scripts for initializing storage (e.g., MinIO buckets)
â”‚   â”œâ”€â”€ jobs/              # Spark job scripts (streaming and batch)
â”‚   â””â”€â”€ producer/          # The Python data streaming server
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile.ingestion   # Dockerfile for the data producer/ingestion image
â”œâ”€â”€ docker-compose.yml     # Docker Compose file for local development
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ requirements.txt       # Python package dependencies
```